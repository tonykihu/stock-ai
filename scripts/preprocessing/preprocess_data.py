import os
import glob
import pandas as pd
import sys

# Add project root to path for utils import
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))
from utils.features import compute_features, TECHNICAL_FEATURES
from utils.tickers import get_tickers_by_country


def load_us_data(file_path):
    """
    Load US stock CSV files generated by yfinance (multi-index format).
    yfinance outputs 3 header rows: Price, Ticker, Date.
    """
    df = pd.read_csv(file_path, header=[0, 1], skiprows=[2])

    # Flatten multi-index columns: keep just the first level (Price, Close, etc.)
    df.columns = [col[0] for col in df.columns]

    # First column is the date (unnamed or 'Price')
    df = df.rename(columns={df.columns[0]: "Date"})
    df["Date"] = pd.to_datetime(df["Date"])

    # Convert numeric columns
    for col in ["Close", "High", "Low", "Open", "Volume"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    return df


def load_kenya_ticker(file_path, ticker_code):
    """
    Extract a single Kenya ticker time-series from NSE CSV.
    Handles multiple formats:
      - nse_2020.csv: DATE, CODE columns, date format dd-Mon-yy
      - Stock AI Development files: DATE/Date, CODE/Code columns, date format YYYY-MM-DD
      - nse_latest.csv: Ticker column (different format, skipped)
    """
    df = pd.read_csv(file_path)

    # Normalize column names to uppercase for matching
    col_map = {c: c.upper() for c in df.columns}
    df = df.rename(columns=col_map)

    if "CODE" not in df.columns:
        print(f"    Skipping {os.path.basename(file_path)} — no CODE column (different format)")
        return pd.DataFrame()

    df = df[df["CODE"] == ticker_code].copy()

    if df.empty:
        return df

    # Standardize column names
    rename_map = {}
    if "DATE" in df.columns:
        rename_map["DATE"] = "Date"
    if "DAY PRICE" in df.columns:
        rename_map["DAY PRICE"] = "Close"
    if "DAY HIGH" in df.columns:
        rename_map["DAY HIGH"] = "High"
    if "DAY LOW" in df.columns:
        rename_map["DAY LOW"] = "Low"
    if "VOLUME" in df.columns:
        rename_map["VOLUME"] = "Volume"

    df = df.rename(columns=rename_map)

    if "Date" in df.columns:
        # Try ISO format first (YYYY-MM-DD), then dd-Mon-yy
        df["Date"] = pd.to_datetime(df["Date"], format="%Y-%m-%d", errors="coerce")
        mask = df["Date"].isna()
        if mask.any():
            df.loc[mask, "Date"] = pd.to_datetime(
                df.loc[mask, "Date"].astype(str), format="%d-%b-%y", errors="coerce"
            )

    # Convert numeric columns (Volume has commas and dashes)
    for col in ["Close", "High", "Low"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    if "Volume" in df.columns:
        df["Volume"] = pd.to_numeric(
            df["Volume"].astype(str).str.replace(",", ""), errors="coerce"
        ).fillna(0).astype(float)
    else:
        df["Volume"] = 0.0

    df = df.sort_values("Date").reset_index(drop=True)
    return df


def preprocess_data():
    """
    Load all US tickers and Kenya data, compute features per ticker,
    and save to data/processed/features.csv.
    """
    os.makedirs("data/processed", exist_ok=True)
    all_frames = []

    # --- Process all US tickers ---
    us_files = glob.glob("data/us/*.csv")
    print(f"Found {len(us_files)} US data files")

    for csv_file in us_files:
        ticker = os.path.basename(csv_file).replace(".csv", "").upper()
        print(f"  Processing {ticker}...")

        df = load_us_data(csv_file)
        if df.empty or "Close" not in df.columns:
            print(f"    Skipping {ticker} — no data")
            continue

        df["Ticker"] = ticker
        df["Market"] = "US"

        # Drop rows missing essential OHLCV data
        df = df.dropna(subset=["Close", "Volume"])

        if len(df) < 60:
            print(f"    Skipping {ticker} — only {len(df)} rows (need 60+ for indicators)")
            continue

        df = compute_features(df)
        all_frames.append(df)
        print(f"    {len(df)} rows processed")

    # --- Process Kenya tickers ---
    kenya_files = glob.glob("data/kenya/*.csv")
    kenya_tickers = get_tickers_by_country("Kenya")

    for kenya_file in kenya_files:
        for ticker_code in kenya_tickers:
            print(f"  Processing Kenya/{ticker_code} from {os.path.basename(kenya_file)}...")
            df = load_kenya_ticker(kenya_file, ticker_code)

            if df.empty or "Close" not in df.columns:
                continue

            df["Ticker"] = ticker_code
            df["Market"] = "Kenya"
            df = df.dropna(subset=["Close"])

            if len(df) < 60:
                print(f"    Skipping {ticker_code} — only {len(df)} rows")
                continue

            df = compute_features(df)
            all_frames.append(df)
            print(f"    {len(df)} rows processed")

    if not all_frames:
        print("ERROR: No data processed!")
        return

    # Combine all tickers and remove duplicates (same Date+Ticker)
    combined = pd.concat(all_frames, ignore_index=True)
    before = len(combined)
    combined = combined.drop_duplicates(subset=["Date", "Ticker"], keep="first")
    dupes = before - len(combined)
    if dupes > 0:
        print(f"  Removed {dupes} duplicate Date+Ticker rows")

    # Keep essential columns + all features
    keep_cols = ["Date", "Ticker", "Market", "Close"] + TECHNICAL_FEATURES
    available = [c for c in keep_cols if c in combined.columns]
    combined[available].to_csv("data/processed/features.csv", index=False)

    print(f"\nFeatures saved to data/processed/features.csv")
    print(f"  Total rows: {len(combined)}")
    print(f"  Tickers: {combined['Ticker'].unique().tolist()}")
    print(f"  Columns: {available}")


if __name__ == "__main__":
    preprocess_data()
